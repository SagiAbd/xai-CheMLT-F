{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b167b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Import your model\n",
    "from Models.chemlt_f_model import DebertaMultiTaskModel\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c983cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = {\n",
    "    0: (\"BACE\", 1, \"classification\"),\n",
    "    1: (\"HIV\", 1, \"classification\"),\n",
    "    2: (\"BBBP\", 1, \"classification\"),\n",
    "    3: (\"ClinTox\", 2, \"classification\"),\n",
    "    4: (\"Tox21\", 12, \"classification\"),\n",
    "    5: (\"MUV\", 17, \"classification\"),\n",
    "    6: (\"SIDER\", 27, \"classification\"),\n",
    "    7: (\"ToxCast\", 617, \"classification\"),\n",
    "    8: (\"Delaney\", 1, \"regression\"),\n",
    "    9: (\"FreeSolv\", 1, \"regression\"),\n",
    "    10: (\"Lipo\", 1, \"regression\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f561a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from Weights/Scaffold_CheMLT-F...\n",
      "768 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2Model were not initialized from the model checkpoint at Weights/Scaffold_CheMLT-F and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.key_proj.weight', 'encoder.layer.0.attention.self.query_proj.bias', 'encoder.layer.0.attention.self.query_proj.weight', 'encoder.layer.0.attention.self.value_proj.bias', 'encoder.layer.0.attention.self.value_proj.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key_proj.bias', 'encoder.layer.1.attention.self.key_proj.weight', 'encoder.layer.1.attention.self.query_proj.bias', 'encoder.layer.1.attention.self.query_proj.weight', 'encoder.layer.1.attention.self.value_proj.bias', 'encoder.layer.1.attention.self.value_proj.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key_proj.bias', 'encoder.layer.2.attention.self.key_proj.weight', 'encoder.layer.2.attention.self.query_proj.bias', 'encoder.layer.2.attention.self.query_proj.weight', 'encoder.layer.2.attention.self.value_proj.bias', 'encoder.layer.2.attention.self.value_proj.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key_proj.bias', 'encoder.layer.3.attention.self.key_proj.weight', 'encoder.layer.3.attention.self.query_proj.bias', 'encoder.layer.3.attention.self.query_proj.weight', 'encoder.layer.3.attention.self.value_proj.bias', 'encoder.layer.3.attention.self.value_proj.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key_proj.bias', 'encoder.layer.4.attention.self.key_proj.weight', 'encoder.layer.4.attention.self.query_proj.bias', 'encoder.layer.4.attention.self.query_proj.weight', 'encoder.layer.4.attention.self.value_proj.bias', 'encoder.layer.4.attention.self.value_proj.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key_proj.bias', 'encoder.layer.5.attention.self.key_proj.weight', 'encoder.layer.5.attention.self.query_proj.bias', 'encoder.layer.5.attention.self.query_proj.weight', 'encoder.layer.5.attention.self.value_proj.bias', 'encoder.layer.5.attention.self.value_proj.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2Model were not initialized from the model checkpoint at Weights/Scaffold_CheMLT-F and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.key_proj.weight', 'encoder.layer.0.attention.self.query_proj.bias', 'encoder.layer.0.attention.self.query_proj.weight', 'encoder.layer.0.attention.self.value_proj.bias', 'encoder.layer.0.attention.self.value_proj.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key_proj.bias', 'encoder.layer.1.attention.self.key_proj.weight', 'encoder.layer.1.attention.self.query_proj.bias', 'encoder.layer.1.attention.self.query_proj.weight', 'encoder.layer.1.attention.self.value_proj.bias', 'encoder.layer.1.attention.self.value_proj.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key_proj.bias', 'encoder.layer.2.attention.self.key_proj.weight', 'encoder.layer.2.attention.self.query_proj.bias', 'encoder.layer.2.attention.self.query_proj.weight', 'encoder.layer.2.attention.self.value_proj.bias', 'encoder.layer.2.attention.self.value_proj.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key_proj.bias', 'encoder.layer.3.attention.self.key_proj.weight', 'encoder.layer.3.attention.self.query_proj.bias', 'encoder.layer.3.attention.self.query_proj.weight', 'encoder.layer.3.attention.self.value_proj.bias', 'encoder.layer.3.attention.self.value_proj.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key_proj.bias', 'encoder.layer.4.attention.self.key_proj.weight', 'encoder.layer.4.attention.self.query_proj.bias', 'encoder.layer.4.attention.self.query_proj.weight', 'encoder.layer.4.attention.self.value_proj.bias', 'encoder.layer.4.attention.self.value_proj.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key_proj.bias', 'encoder.layer.5.attention.self.key_proj.weight', 'encoder.layer.5.attention.self.query_proj.bias', 'encoder.layer.5.attention.self.query_proj.weight', 'encoder.layer.5.attention.self.value_proj.bias', 'encoder.layer.5.attention.self.value_proj.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded from model.safetensors\n",
      "✅ Tokenizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_dir):\n",
    "    \"\"\"Load CheMLT-F model from HuggingFace format directory.\"\"\"\n",
    "    \n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    \n",
    "    # Initialize model architecture\n",
    "    model = DebertaMultiTaskModel(\n",
    "        model_path1=model_dir,\n",
    "        model_path2=model_dir,\n",
    "        num_labels_list=[1, 1, 1, 2, 12, 17, 27, 617, 1, 1, 1],\n",
    "        problem_type_list=[\n",
    "            \"classification\", \"classification\", \"classification\",\n",
    "            \"classification\", \"classification\", \"classification\",\n",
    "            \"classification\", \"classification\",\n",
    "            \"regression\", \"regression\", \"regression\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    try:\n",
    "        from safetensors.torch import load_file\n",
    "        state_dict = load_file(f\"{model_dir}/model.safetensors\")\n",
    "        print(\"✅ Loaded from model.safetensors\")\n",
    "    except:\n",
    "        state_dict = torch.load(f\"{model_dir}/pytorch_model.bin\", map_location=\"cpu\")\n",
    "        print(\"✅ Loaded from pytorch_model.bin\")\n",
    "    \n",
    "    # Filter out encoder2 (for SMILES-only)\n",
    "    filtered = {k: v for k, v in state_dict.items() if not k.startswith(\"encoder2.\")}\n",
    "    model.load_state_dict(filtered, strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    print(\"✅ Tokenizer loaded\\n\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the model\n",
    "MODEL_DIR = \"Weights/Scaffold_CheMLT-F\"\n",
    "model, tokenizer = load_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bd7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheMLTWrapper(torch.nn.Module):\n",
    "    \"\"\"Wrapper for CheMLT model to work with Captum.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, task_index, label_index=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The CheMLT multitask model\n",
    "            task_index: Which task to interpret (0-10)\n",
    "            label_index: For multi-label tasks, which label to interpret (default: 0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.task_index = task_index\n",
    "        self.label_index = label_index\n",
    "        self.task_name, self.num_labels, self.task_type = TASKS[task_index]\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass returning scalar prediction for the target label.\n",
    "        \"\"\"\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            input_ids2=None,\n",
    "            attention_mask2=None,\n",
    "            input_ids3=None,\n",
    "            attention_mask3=None,\n",
    "            task_index=self.task_index\n",
    "        )\n",
    "        \n",
    "        logits = outputs[\"logits\"]\n",
    "        \n",
    "        # For classification, return probability\n",
    "        if self.task_type == \"classification\":\n",
    "            probs = torch.sigmoid(logits)\n",
    "            if self.num_labels == 1:\n",
    "                return probs.squeeze(-1)\n",
    "            else:\n",
    "                return probs[:, self.label_index]\n",
    "        # For regression, return raw value\n",
    "        else:\n",
    "            return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d57415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_smiles(model, tokenizer, smiles, task_index, label_index=0):\n",
    "    \"\"\"\n",
    "    Get prediction and confidence for a SMILES string.\n",
    "    \n",
    "    Returns:\n",
    "        pred_value: The prediction (probability or regression value)\n",
    "        pred_label: Human-readable prediction\n",
    "        token_ids: Token IDs for interpretation\n",
    "        tokens: Actual tokens\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        smiles,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Create wrapper\n",
    "    wrapper = CheMLTWrapper(model, task_index, label_index)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred_value = wrapper(input_ids, attention_mask).item()\n",
    "    \n",
    "    # Format prediction\n",
    "    task_name, num_labels, task_type = TASKS[task_index]\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        pred_label = f\"Active (prob={pred_value:.3f})\" if pred_value > 0.5 else f\"Inactive (prob={pred_value:.3f})\"\n",
    "    else:\n",
    "        pred_label = f\"Value={pred_value:.3f}\"\n",
    "    \n",
    "    return pred_value, pred_label, input_ids, attention_mask, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e40416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attributions(model, input_ids, attention_mask, task_index, label_index=0):\n",
    "    \"\"\"\n",
    "    Compute token attributions using Layer Integrated Gradients.\n",
    "    \n",
    "    Returns:\n",
    "        attributions: Attribution scores for each token\n",
    "        delta: Approximation error\n",
    "    \"\"\"\n",
    "    # Create wrapper\n",
    "    wrapper = CheMLTWrapper(model, task_index, label_index)\n",
    "    \n",
    "    # Get embedding layer\n",
    "    embeddings = model.encoder1.embeddings\n",
    "    \n",
    "    # Create Layer Integrated Gradients\n",
    "    lig = LayerIntegratedGradients(wrapper, embeddings)\n",
    "    \n",
    "    # Create baseline (PAD token)\n",
    "    baseline_ids = torch.zeros_like(input_ids).long()\n",
    "    baseline_ids[:] = tokenizer.pad_token_id\n",
    "    baseline_mask = torch.zeros_like(attention_mask)\n",
    "    \n",
    "    # Compute attributions\n",
    "    attributions, delta = lig.attribute(\n",
    "        inputs=(input_ids, attention_mask),\n",
    "        baselines=(baseline_ids, baseline_mask),\n",
    "        return_convergence_delta=True,\n",
    "        n_steps=50\n",
    "    )\n",
    "    \n",
    "    # Sum across embedding dimension\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    return attributions, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d248c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attributions(tokens, attributions, pred_label, smiles):\n",
    "    \"\"\"\n",
    "    Create HTML visualization of token attributions.\n",
    "    \"\"\"\n",
    "    # Filter out padding tokens\n",
    "    valid_indices = [i for i, token in enumerate(tokens) if token != tokenizer.pad_token]\n",
    "    tokens = [tokens[i] for i in valid_indices]\n",
    "    attributions = attributions[valid_indices]\n",
    "    \n",
    "    # Normalize attributions for visualization\n",
    "    attr_sum = np.abs(attributions).sum()\n",
    "    normalized_attrs = attributions / attr_sum if attr_sum != 0 else attributions\n",
    "    \n",
    "    # Display header\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SMILES: {smiles}\")\n",
    "    print(f\"Prediction: {pred_label}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create a single visualization record with all tokens\n",
    "    vis_record = viz.VisualizationDataRecord(\n",
    "        word_attributions=normalized_attrs.tolist(),\n",
    "        pred_prob=0.0,  # Scalar float\n",
    "        pred_class=pred_label,\n",
    "        true_class=\"\",\n",
    "        attr_class=smiles,\n",
    "        attr_score=float(np.abs(normalized_attrs).sum()),  # Scalar sum of attributions\n",
    "        raw_input_ids=tokens,\n",
    "        convergence_score=0.0\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    html = viz.visualize_text([vis_record])\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d74117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# Task: BBBP - Blood-Brain Barrier Permeability\n",
      "################################################################################\n",
      "\n",
      "\n",
      "--- Analyzing: Caffeine ---\n",
      "\n",
      "================================================================================\n",
      "SMILES: CN1C=NC2=C1C(=O)N(C(=O)N2C)C\n",
      "Prediction: Active (prob=0.982)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b></b></text></td><td><text style=\"padding-right:2em\"><b>Active (prob=0.982) (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>CN1C=NC2=C1C(=O)N(C(=O)N2C)C</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> CN                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> =                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> NC                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> =                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (=                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> N                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (=                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> N                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Contributing Tokens:\n",
      "  CN         | Attribution: -0.4153 → Inactive\n",
      "  O          | Attribution: -0.3787 → Inactive\n",
      "  2          | Attribution: +0.3639 → Active\n",
      "  =          | Attribution: +0.2674 → Active\n",
      "  1          | Attribution: +0.2663 → Active\n",
      "\n",
      "Convergence Delta: -0.011888\n",
      "\n",
      "--- Analyzing: Aspirin ---\n",
      "\n",
      "================================================================================\n",
      "SMILES: CC(=O)Oc1ccccc1C(=O)O\n",
      "Prediction: Inactive (prob=0.369)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b></b></text></td><td><text style=\"padding-right:2em\"><b>Inactive (prob=0.369) (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>CC(=O)Oc1ccccc1C(=O)O</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> CC                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (=                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Oc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ccccc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (=                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Contributing Tokens:\n",
      "  O          | Attribution: -0.4621 → Inactive\n",
      "  (=         | Attribution: -0.4348 → Inactive\n",
      "  O          | Attribution: -0.3719 → Inactive\n",
      "  1          | Attribution: +0.3693 → Active\n",
      "  1          | Attribution: +0.2869 → Active\n",
      "\n",
      "Convergence Delta: -0.009743\n",
      "\n",
      "--- Analyzing: Diazepam ---\n",
      "\n",
      "================================================================================\n",
      "SMILES: CN1C(=O)CN=C(c2ccccc2)c3cc(Cl)ccc13\n",
      "Prediction: Active (prob=0.981)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b></b></text></td><td><text style=\"padding-right:2em\"><b>Active (prob=0.981) (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>CN1C(=O)CN=C(c2ccccc2)c3cc(Cl)ccc13</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> CN                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (=                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> O                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> CN                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> =                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> C                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> c                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ccccc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> c                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Cl                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> )                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ccc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 13                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Contributing Tokens:\n",
      "  [CLS]      | Attribution: +0.4549 → Active\n",
      "  ccccc      | Attribution: +0.3913 → Active\n",
      "  2          | Attribution: +0.2779 → Active\n",
      "  1          | Attribution: +0.2774 → Active\n",
      "  O          | Attribution: -0.2770 → Inactive\n",
      "\n",
      "Convergence Delta: -0.024323\n"
     ]
    }
   ],
   "source": [
    "# Test molecules\n",
    "test_molecules = {\n",
    "    \"Caffeine\": \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",  # Expected: High BBB permeability\n",
    "    \"Aspirin\": \"CC(=O)Oc1ccccc1C(=O)O\",  # Expected: Low BBB permeability\n",
    "    \"Diazepam\": \"CN1C(=O)CN=C(c2ccccc2)c3cc(Cl)ccc13\",  # Expected: High BBB permeability\n",
    "}\n",
    "\n",
    "task_idx = 2  # BBBP task\n",
    "task_name = TASKS[task_idx][0]\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(f\"# Task: {task_name} - Blood-Brain Barrier Permeability\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "for drug_name, smiles in test_molecules.items():\n",
    "    print(f\"\\n--- Analyzing: {drug_name} ---\")\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_value, pred_label, input_ids, attention_mask, tokens = predict_smiles(\n",
    "        model, tokenizer, smiles, task_idx\n",
    "    )\n",
    "    \n",
    "    # Compute attributions\n",
    "    attributions, delta = compute_attributions(\n",
    "        model, input_ids, attention_mask, task_idx\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    html = visualize_attributions(tokens, attributions, pred_label, smiles)\n",
    "    \n",
    "    # Show top contributing tokens\n",
    "    valid_indices = [i for i, token in enumerate(tokens) if token != tokenizer.pad_token]\n",
    "    valid_tokens = [tokens[i] for i in valid_indices]\n",
    "    valid_attrs = attributions[valid_indices]\n",
    "    \n",
    "    # Sort by absolute attribution\n",
    "    sorted_indices = np.argsort(np.abs(valid_attrs))[::-1]\n",
    "    \n",
    "    print(\"\\nTop 5 Contributing Tokens:\")\n",
    "    for i in sorted_indices[:5]:\n",
    "        token = valid_tokens[i]\n",
    "        attr = valid_attrs[i]\n",
    "        direction = \"→ Active\" if attr > 0 else \"→ Inactive\"\n",
    "        print(f\"  {token:10s} | Attribution: {attr:+.4f} {direction}\")\n",
    "    \n",
    "    print(f\"\\nConvergence Delta: {delta.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15bb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
